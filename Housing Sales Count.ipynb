{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c37d54-d990-4f91-9e84-a22d3af9eff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Housing Sales Count\n",
    "This notebook imports Housing Sales Count data from [Zillow](https://files.zillowstatic.com/research/public_csvs/sales_count_now/Metro_sales_count_now_uc_sfrcondo_month.csv?t=1746553689) and performs the following operations:\n",
    "\n",
    "- **Data Transformation:** Converts date columns into rows for improved accessibility.\n",
    "- **Data Cleaning:** Fills missing StateName values when the region type is categorized as Country.\n",
    "- **Data Storage:** Saves the processed data to an existing Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f873bc5-8e2c-45b7-a976-a9706e1e05d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import PySpark SQL Functions"
    }
   },
   "outputs": [],
   "source": [
    "# Importing all functions from pyspark.sql.functions module\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a893564a-8c35-4390-acb5-a86f993f207a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reshape and Save Metro Sales Data to Table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "data_path = \"/Volumes/generaldata/dataanalysis/upload/Metro_sales_count_now_uc_sfrcondo_month.csv\"\n",
    "\n",
    "# Define catalog, schema, and table names for saving the DataFrame\n",
    "u_catalog = \"generaldata\"\n",
    "u_schema = \"dataanalysis\"\n",
    "u_table = \"Metro_Sales_Count\"\n",
    "\n",
    "# Read the CSV file into a DataFrame with header and inferred schema\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows where RegionType is \"country\"\n",
    "df = df.filter(df.RegionType != \"country\")\n",
    "\n",
    "# Initialize a list to store column names that contain \"20\"\n",
    "col_to_row_list = []\n",
    "for col in df.columns:\n",
    "    if \"20\" in col:\n",
    "        col_to_row_list.append(col)\n",
    "\n",
    "# Transform the DataFrame from wide to long format using melt\n",
    "sales_df = df.melt(\n",
    "    ids=[\"RegionID\", \"SizeRank\", \"RegionName\", \"RegionType\", \"StateName\"], values=col_to_row_list,\n",
    "    variableColumnName=\"Date\", valueColumnName=\"Count\"\n",
    ")\n",
    "\n",
    "# Drop the original columns that were melted\n",
    "sales_df = sales_df.drop(*col_to_row_list)\n",
    "\n",
    "# Fill missing values in 'StateName' column with 'USA'\n",
    "sales_df = sales_df.fillna({'StateName': 'USA'})\n",
    "\n",
    "# Save the transformed DataFrame as a table in the specified catalog and schema\n",
    "sales_df.write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"overwriteSchema\", \"true\")\\\n",
    "        .saveAsTable(f\"{u_catalog}.{u_schema}.{u_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Housing Sales Count",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
